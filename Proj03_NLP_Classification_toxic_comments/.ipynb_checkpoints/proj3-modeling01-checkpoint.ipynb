{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import of library","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"#Importing libraries\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score, f1_score","metadata":{"execution":{"iopub.status.busy":"2023-04-29T10:37:35.656819Z","iopub.execute_input":"2023-04-29T10:37:35.657409Z","iopub.status.idle":"2023-04-29T10:37:36.144878Z","shell.execute_reply.started":"2023-04-29T10:37:35.657354Z","shell.execute_reply":"2023-04-29T10:37:36.143411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train, test & split of data","metadata":{}},{"cell_type":"code","source":"#Importing dataframe\nworking_df=pd.read_csv('/kaggle/input/project-3/working_df.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-29T10:41:03.922999Z","iopub.execute_input":"2023-04-29T10:41:03.923495Z","iopub.status.idle":"2023-04-29T10:41:04.019243Z","shell.execute_reply.started":"2023-04-29T10:41:03.923434Z","shell.execute_reply":"2023-04-29T10:41:04.017964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Assigning X and y variables\nX=working_df['text_translated']\ny=working_df['hateful']\n\n#Splitting the data into train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=1, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T10:41:27.360602Z","iopub.execute_input":"2023-04-29T10:41:27.361017Z","iopub.status.idle":"2023-04-29T10:41:27.390558Z","shell.execute_reply.started":"2023-04-29T10:41:27.360984Z","shell.execute_reply":"2023-04-29T10:41:27.389251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions to be use (or shld use a python script to import functions)","metadata":{}},{"cell_type":"markdown","source":"# Approach 1A: TF-IDF vectorization + Multinomial Naive Bayes ","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing & Vectorizing","metadata":{}},{"cell_type":"markdown","source":"Rationale: \n- TF-IDF vectorization is selected for the first approach as it assigns heavier weight to less frequent tokens and less weight to more frequent tokens which might provide a better basis for training a more effective model.  ","metadata":{}},{"cell_type":"markdown","source":"The data will undergo preprocessing with TFIDF vectorizing with:\n- the use of standard english stopwords filter\n- maximum features of up till 10,000\n- maximum appearence of a word across 95% of all documents\n- minimum appearence of a word across 1% of all documents","metadata":{}},{"cell_type":"code","source":"#Instantiate of TF-IDF vectorizer\ntvec1A=TfidfVectorizer(stop_words='english', max_features=1000, max_df=0.95, min_df=0.01)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T10:41:38.281008Z","iopub.execute_input":"2023-04-29T10:41:38.281948Z","iopub.status.idle":"2023-04-29T10:41:38.287155Z","shell.execute_reply.started":"2023-04-29T10:41:38.281903Z","shell.execute_reply":"2023-04-29T10:41:38.285713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fit and transform of data\nX_train_tvec1A=tvec1A.fit_transform(X_train)\nX_test_tvec1A=tvec1A.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T10:42:04.998357Z","iopub.execute_input":"2023-04-29T10:42:04.998836Z","iopub.status.idle":"2023-04-29T10:42:05.395943Z","shell.execute_reply.started":"2023-04-29T10:42:04.998795Z","shell.execute_reply":"2023-04-29T10:42:05.394712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"Rationale:\n- Multinomial Naive Bayes is selected due to its relative speed in training and since TF-IDF vectorizer is used which provides fractional counts of the tokens. Despite the unsatisfied assumption of independent tokens, it ususally still provide decent results.","metadata":{}},{"cell_type":"code","source":"# Instantiate MultinomialNB model\nnb1A=MultinomialNB()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T10:42:09.520615Z","iopub.execute_input":"2023-04-29T10:42:09.521785Z","iopub.status.idle":"2023-04-29T10:42:09.526184Z","shell.execute_reply.started":"2023-04-29T10:42:09.521737Z","shell.execute_reply":"2023-04-29T10:42:09.525137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train of model\nmodel1A=nb1A.fit(X_train_tvec1A, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T10:42:11.638995Z","iopub.execute_input":"2023-04-29T10:42:11.640323Z","iopub.status.idle":"2023-04-29T10:42:11.651889Z","shell.execute_reply.started":"2023-04-29T10:42:11.640261Z","shell.execute_reply":"2023-04-29T10:42:11.650722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Getting predictions of train and test data\ntrain_pred_model1A=model1A.predict(X_train_tvec1A)\ntest_pred_model1A=model1A.predict(X_test_tvec1A)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T10:42:16.727082Z","iopub.execute_input":"2023-04-29T10:42:16.727539Z","iopub.status.idle":"2023-04-29T10:42:16.735830Z","shell.execute_reply.started":"2023-04-29T10:42:16.727496Z","shell.execute_reply":"2023-04-29T10:42:16.734067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"markdown","source":"Given how the dataset is imbalanced, we will explore other metrics beside accuracy score, specifically:\n\n- Recall **(Higher consideration)** \n    - Measures type 1 errorSince we want to ensure that a hateful comment will be successfully targeted whenever it is present.\n- Precision **(Lesser considertion)** How accurate the detected hateful comment predictions are)\n- F1 score (Balance between Precision and Recall for overall performance)","metadata":{}},{"cell_type":"code","source":"#Creating and visualising the confusion matrix on predictions from the test data\ncm1A=confusion_matrix(y_test,test_pred_model1A)\n\n#Plot confusion matrix\ndisp1A = ConfusionMatrixDisplay(confusion_matrix=cm1A, display_labels=['Non-hateful', 'Hateful'])\ndisp1A.plot();","metadata":{"execution":{"iopub.status.busy":"2023-04-29T10:48:17.104564Z","iopub.execute_input":"2023-04-29T10:48:17.104978Z","iopub.status.idle":"2023-04-29T10:48:17.406915Z","shell.execute_reply.started":"2023-04-29T10:48:17.104941Z","shell.execute_reply":"2023-04-29T10:48:17.405552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score_summary(y_true,y_pred):\n    print(f'Recall score:{recall_score(y_true, y_pred)}')\n    print(f'Precision score:{precision_score(y_true, y_pred)}')\n    print(f'F1_score:{f1_score(y_true, y_pred)}')","metadata":{"execution":{"iopub.status.busy":"2023-04-29T10:43:09.357015Z","iopub.execute_input":"2023-04-29T10:43:09.358244Z","iopub.status.idle":"2023-04-29T10:43:09.363361Z","shell.execute_reply.started":"2023-04-29T10:43:09.358190Z","shell.execute_reply":"2023-04-29T10:43:09.362391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Summary of train data scores\nscore_summary(y_train,train_pred_model1A)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T10:43:24.590503Z","iopub.execute_input":"2023-04-29T10:43:24.590926Z","iopub.status.idle":"2023-04-29T10:43:24.634418Z","shell.execute_reply.started":"2023-04-29T10:43:24.590889Z","shell.execute_reply":"2023-04-29T10:43:24.633067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Summary of test data scores\nscore_summary(y_test,test_pred_model1A)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T10:45:44.979522Z","iopub.execute_input":"2023-04-29T10:45:44.980114Z","iopub.status.idle":"2023-04-29T10:45:45.002548Z","shell.execute_reply.started":"2023-04-29T10:45:44.980059Z","shell.execute_reply":"2023-04-29T10:45:45.001289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion:\n- Very poor recall score","metadata":{}},{"cell_type":"markdown","source":"# PIPELINE CONSTRUCTION STRUCTURE","metadata":{}},{"cell_type":"markdown","source":"## Approach XX: vectorizer+model","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing & Vectorizing","metadata":{}},{"cell_type":"markdown","source":"Rationale:","metadata":{}},{"cell_type":"markdown","source":"Parameters of preprocess and vectorizing:","metadata":{}},{"cell_type":"code","source":"#Setting up pipeline of respective Vectorizer and Model\npipeXX = Pipeline([('cvec', CountVectorizer()),('nb', MultinomialNB())])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-04-29T08:02:39.067863Z","iopub.execute_input":"2023-04-29T08:02:39.068368Z","iopub.status.idle":"2023-04-29T08:02:39.073738Z","shell.execute_reply.started":"2023-04-29T08:02:39.068304Z","shell.execute_reply":"2023-04-29T08:02:39.072438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Countvectorize/TFIDF params\n'cvec__max_features': [2_000, 3_000, 4_000, 5_000],\n'cvec__min_df': [2, 3],\n'cvec__max_df': [.9, .95],\n'cvec__ngram_range': [(1,1), (1,2)\n                          \n#Mulitnomial NB params\n'nb__alpha': np.linspace[1,10,3]\n                      \n#Logistic Regression\n'lr__C': np.linspace[1,10,3]\n'lr__penalty': np.logspace(-3,0,3)                    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeXX_params={\n    'cvec__max_features': [2_000, 3_000, 4_000, 5_000],\n    'cvec__min_df': [2, 3],\n    'cvec__max_df': [.9, .95],\n    'cvec__ngram_range': [(1,1), (1,2)]","metadata":{},"execution_count":null,"outputs":[]}]}